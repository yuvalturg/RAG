version: '3.8'

services:

  # NOTE: Ollama is now expected to run on the host machine for better performance
  # To run Ollama locally:
  #   1. Install Ollama: https://ollama.ai/download
  #   2. Start Ollama service (usually automatic on install)
  #   3. Pull the model: ollama pull llama3.2:3b-instruct-fp16
  # 
  # The containers will connect to Ollama on the host:
  #   - macOS/Windows: Uses host.docker.internal:11434 (works with Podman)
  #   - Linux: May need to set OLLAMA_URL=http://172.17.0.1:11434 or use --network=host

  # Llama Stack server
  llamastack:
    image: llamastack/distribution-ollama:0.2.9
    platform: linux/amd64
    container_name: rag-llamastack
    restart: on-failure:50
    environment:
      INFERENCE_MODEL: "llama3.2:3b-instruct-fp16"
      # For macOS/Podman: Use host.docker.internal which works better than host.containers.internal
      # For Linux: Use 172.17.0.1 or host.containers.internal
      OLLAMA_URL: "${OLLAMA_URL:-http://host.docker.internal:11434}"
      TAVILY_SEARCH_API_KEY: "${TAVILY_SEARCH_API_KEY:-}"  # Set this if you have a Tavily API key
    ports:
      - "8321:8321"
    volumes:
      - llamastack_data:/root/.llama
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8321/"]
      interval: 30s
      timeout: 10s
      retries: 10
      start_period: 120s  # Give more time for startup
    networks:
      - rag-network


  # RAG Ingestion Service
  rag-ingestion:
    platform: linux/amd64
    build:
      context: ../../ingestion-service
      dockerfile: Containerfile
    container_name: rag-ingestion
    depends_on:
      llamastack:
        condition: service_started
    environment:
      INGESTION_CONFIG: "/config/ingestion-config.yaml"
    volumes:
      - ./ingestion-config.yaml:/config/ingestion-config.yaml:ro
    networks:
      - rag-network
    restart: "no"  # Run once and exit

  # RAG UI Frontend
  rag-ui:
    platform: linux/amd64
    build:
      context: ../../frontend
      dockerfile: Containerfile
    container_name: rag-ui
    depends_on:
      llamastack:
        condition: service_started  # Changed from service_healthy
    environment:
      LLAMA_STACK_ENDPOINT: "http://llamastack:8321"
      TAVILY_SEARCH_API_KEY: "${TAVILY_SEARCH_API_KEY:-}"  # Set this if you have a Tavily API key
      RAG_QUESTION_SUGGESTIONS: |
        {
          "hr-vector-db-v1-0": [
            "What benefits does FantaCo provide?",
            "How many vacation days do employees get?",
            "What is the parental leave policy?",
            "What are the retirement benefits?",
            "How do I enroll in benefits?",
            "What is the employee assistance program?"
          ],
          "legal-vector-db-v1-0": [
            "What are the key contract terms?",
            "What is the liability clause?",
            "What are the termination conditions?",
            "What are the intellectual property rights?",
            "What is the dispute resolution process?",
            "What are the compliance requirements?"
          ],
          "sales-vector-db-v1-0": [
            "What is the sales process?",
            "How do I qualify leads?",
            "What are the pricing strategies?",
            "What is the commission structure?",
            "How do I handle customer objections?",
            "What are the territory assignments?"
          ],
          "procurement-vector-db-v1-0": [
            "What is the procurement process?",
            "How do I submit a purchase request?",
            "What are the approval requirements?",
            "Who are the approved vendors?",
            "What is the purchasing policy?",
            "How do I track my order?"
          ],
          "techsupport-vector-db-v1-0": [
            "How do I install CloudSync on Mac?",
            "How do I install CloudSync on Windows?",
            "How do I sync files between devices?",
            "How do I troubleshoot CloudSync sync issues?",
            "How do I install Linux on TechGear Pro Laptop?",
            "Where can I find video drivers for TechGear Pro?"
          ]
        }
    ports:
      - "8501:8501"
    networks:
      - rag-network
    restart: unless-stopped


volumes:
  llamastack_data:
    driver: local

networks:
  rag-network:
    driver: bridge